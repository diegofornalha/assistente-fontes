import os
from pathlib import Path
from dotenv import load_dotenv
from llama_index.core import (
    SimpleDirectoryReader,
    GPTVectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings,
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Carrega variÃ¡veis do .env
load_dotenv()

# Caminhos absolutos (nÃ£o dependem do diretÃ³rio atual ao rodar o uvicorn)
BASE_DIR = Path(__file__).resolve().parent.parent      # /assistente-fontes
BACKEND_DIR = Path(__file__).resolve().parent          # /assistente-fontes/backend-dados

# ðŸ“ DiretÃ³rio e caminho do Ã­ndice
INDEX_DIR = str(BASE_DIR / "storage")
INDEX_FILE = str(Path(INDEX_DIR) / "index.json")
TRANSCRICOES_PATH = str(BACKEND_DIR / "transcricoes.txt")

# ðŸ¤– Define o modelo de embedding (sentence-transformers local, gratuito)
# Usa modelo multilÃ­ngue otimizado para portuguÃªs
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

def load_or_build_index():
    """Carrega o Ã­ndice existente ou cria um novo a partir de transcricoes.txt."""
    if os.path.exists(INDEX_FILE):
        print("ðŸ“ Ãndice encontrado. Carregando do disco...")
        storage_context = StorageContext.from_defaults(persist_dir=INDEX_DIR)
        return load_index_from_storage(storage_context)
    else:
        print("âš™ï¸ Ãndice nÃ£o encontrado. Construindo novo...")
        docs = SimpleDirectoryReader(input_files=[TRANSCRICOES_PATH]).load_data()
        index = GPTVectorStoreIndex.from_documents(docs)
        index.storage_context.persist(persist_dir=INDEX_DIR)
        print(f"âœ… Ãndice construÃ­do com {len(docs)} documentos.")
        return index

# âš¡ Inicializa o Ã­ndice na importaÃ§Ã£o deste mÃ³dulo
index = load_or_build_index()

def retrieve_relevant_context(
    question: str,
    top_k: int = 3,
    chunk_size: int = 512
) -> str:
    """
    Busca no Ã­ndice atÃ© `top_k` trechos que respondam Ã  `question`.
    Usa `chunk_size` para controlar o tamanho dos blocos de texto.
    Retorna string vazia se nÃ£o encontrar algo relevante.
    """
    # DEBUG: confira nos logs qual pergunta chegou
    print("ðŸ”Ž DEBUG â€” Pergunta para contexto:", question)

    # Usa retriever em vez de query_engine (nÃ£o precisa de LLM)
    retriever = index.as_retriever(similarity_top_k=top_k)
    nodes = retriever.retrieve(question)

    # Combina os textos dos nodes recuperados
    if not nodes:
        print("ðŸ”Ž DEBUG â€” Nenhum nÃ³ recuperado")
        return ""

    response_str = "\n\n".join([node.text for node in nodes])
    # DEBUG: confira o texto bruto retornado
    print("ðŸ”Ž DEBUG â€” Contexto bruto retornado:", response_str[:200] + "...")

    lower = response_str.lower()
    # se vazio ou sem sentido
    if not lower or lower in ("none", "null"):
        print("ðŸ”Ž DEBUG â€” Contexto vazio apÃ³s normalizaÃ§Ã£o")
        return ""

    # bloqueia respostas genÃ©ricas
    for frase in ("nÃ£o tenho certeza", "desculpe", "nÃ£o sei"):
        if frase in lower:
            print("ðŸ”Ž DEBUG â€” Contexto bloqueado por frase de incerteza")
            return ""

    # filtra termos fora de escopo
    proibidos = [
        "instagram", "vÃ­deos para instagram", "celular para gravar", "smartphone",
        "tiktok", "post viral", "gravar vÃ­deos", "microfone", "cÃ¢mera",
        "ediÃ§Ã£o de vÃ­deo", "hashtags", "stories", "marketing de conteÃºdo",
        "produÃ§Ã£o de vÃ­deo", "influencer"
    ]
    if any(tp in lower for tp in proibidos):
        print("ðŸ”Ž DEBUG â€” Contexto bloqueado por termo proibido")
        return ""

    # DEBUG: contexto aprovado
    print("ðŸ”Ž DEBUG â€” Contexto final aceito:", response_str)
    return response_str
