import os
import shutil
from llama_index.core import (
    SimpleDirectoryReader,
    GPTVectorStoreIndex,
    Settings
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# Caminho de saÃ­da
INDEX_DIR = "storage"

# Apaga Ã­ndice antigo (importante!)
if os.path.exists(INDEX_DIR):
    print("ğŸ§¹ Limpando Ã­ndice anterior...")
    shutil.rmtree(INDEX_DIR)

# Define o modelo de embedding (sentence-transformers local, gratuito)
# Usa modelo multilÃ­ngue otimizado para portuguÃªs
Settings.embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# LÃª os dados da transcriÃ§Ã£o
print("ğŸ“„ Lendo o arquivo transcricoes.txt...")
documents = SimpleDirectoryReader(input_files=["transcricoes.txt"]).load_data()

# Gera o Ã­ndice
print("âš™ï¸ Gerando o Ã­ndice vetorial...")
index = GPTVectorStoreIndex.from_documents(documents)

# Persiste no diretÃ³rio
print(f"ğŸ’¾ Salvando Ã­ndice em: {INDEX_DIR}")
index.storage_context.persist(persist_dir=INDEX_DIR)

print("âœ… Ãndice criado com sucesso.")
