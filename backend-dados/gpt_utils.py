import os
import re
import random
from anthropic import Anthropic
from dotenv import load_dotenv

# Carrega vari√°veis do .env
load_dotenv()

TRANSCRIPTS_PATH = os.path.join(os.path.dirname(__file__), "transcricoes.txt")

# Aceita token via MINIMAX_API_KEY (padr√£o) ou ANTHROPIC_AUTH_TOKEN (fallback).
# Obs: o backend usa base_url da MiniMax, ent√£o ambos apontam para o mesmo token JWT.
_API_KEY = os.getenv("MINIMAX_API_KEY") or os.getenv("ANTHROPIC_AUTH_TOKEN")

# Configura√ß√£o Minimax via API compat√≠vel com Anthropic
client = Anthropic(
    base_url="https://api.minimax.io/anthropic",
    api_key=_API_KEY
)

OUT_OF_SCOPE_MSG = (
    "Desculpe, ainda n√£o tenho informa√ß√µes suficientes sobre esse tema espec√≠fico. "
    "Por favor, envie outra pergunta ou consulte a documenta√ß√£o dispon√≠vel."
)

CONTINUE_GUARDRAILS = (
    "IMPORTANTE (tamanho e continuidade): "
    "Se a resposta ficar longa, entregue em partes. "
    "Conclua a PARTE atual de forma completa (n√£o deixe itens numerados/bullets pela metade) "
    "e finalize com a frase: 'Quer que eu continue?' "
    "N√£o continue automaticamente sem o Doutor(a) pedir."
)

def _looks_truncated(text: str) -> bool:
    """Heur√≠stica simples para detectar respostas cortadas (ex.: termina em '2.' ou palavra incompleta)."""
    if not isinstance(text, str):
        return False
    t = text.strip()
    if not t:
        return False

    # Normaliza quebras HTML comuns
    t = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n").strip()

    # Termina com marcador de lista sem conte√∫do
    for suffix in ("-", "‚Ä¢", "*", "1.", "2.", "3.", "4.", "5.", "6.", "7.", "8.", "9.", "10."):
        if t.endswith(suffix):
            return True

    # √öltimo caractere alfanum√©rico sem pontua√ß√£o (pode indicar corte)
    last = t[-1]
    if last.isalnum():
        # Se a √∫ltima "linha" for muito curta e sem pontua√ß√£o, √© suspeito
        last_line = t.splitlines()[-1].strip()
        if last_line and last_line[-1].isalnum() and len(last_line) <= 12:
            return True
    return False

def _should_offer_continue(text: str) -> bool:
    """
    Se a resposta ficou "longa", oferecemos continua√ß√£o mesmo que n√£o pare√ßa truncada.
    A ideia √© padronizar a experi√™ncia: longos conte√∫dos viram "em partes".
    """
    if not isinstance(text, str):
        return False
    t = text.strip()
    if not t:
        return False
    if "Quer que eu continue?" in t:
        return False

    # Normaliza quebras HTML comuns
    t_plain = t.replace("<br>", "\n").replace("<br/>", "\n").replace("<br />", "\n")

    # Heur√≠sticas: tamanho + densidade de estrutura
    if len(t_plain) >= 1400:
        return True

    lines = [ln.strip() for ln in t_plain.splitlines() if ln.strip()]
    if len(lines) >= 18:
        return True

    bullet_lines = sum(1 for ln in lines if ln.startswith(("-", "*", "‚Ä¢")) or re.match(r"^\d+\.\s+", ln))
    if bullet_lines >= 8:
        return True

    headings = sum(1 for ln in lines if re.match(r"^#{1,6}\s+", ln))
    if headings >= 3:
        return True

    return False

def _append_continue_hint(text: str) -> str:
    hint = "\n\n---\n\n**Parece que ainda tem mais conte√∫do. Quer que eu continue?**"
    if not isinstance(text, str):
        return hint
    if "Quer que eu continue?" in text:
        return text
    return f"{text.rstrip()}{hint}"

GREETINGS = [
    "Ol√°! Como posso ajudar voc√™ hoje?",
    "Oi! Tudo bem? Em que posso ajudar?",
    "Bem-vindo(a) de volta! Como posso ajudar?",
    "Ol√°! Estou aqui para ajudar."
]

CLOSINGS = [
    "Ficou com alguma d√∫vida?",
    "Deseja aprofundar algum ponto ou fazer outra pergunta?",
    "Se quiser, escolha uma op√ß√£o r√°pida abaixo ou pergunte de novo!",
    "Se quiser fazer outra pergunta, √© s√≥ pedir.",
    "Essa resposta foi √∫til? Clique em üëç ou üëé."
]

# Estrutura de m√≥dulos/aulas removida - sistema agora usa base de conhecimento do transcricoes.txt

def formatar_historico_para_prompt(history):
    """
    Formata o hist√≥rico de conversa para ser inclu√≠do no prompt da API.
    Remove HTML e campos desnecess√°rios, mantendo apenas user/ai.
    """
    if not history or not isinstance(history, list):
        return "Nenhuma conversa anterior."

    import re
    linhas = []
    for i, item in enumerate(history[-5:]):  # Pega apenas √∫ltimas 5 intera√ß√µes para n√£o estourar tokens
        user_msg = item.get('user', '')
        ai_msg = item.get('ai', '')

        # Remove tags HTML das mensagens
        user_msg = re.sub(r'<[^>]+>', '', user_msg).strip()
        ai_msg = re.sub(r'<[^>]+>', '', ai_msg).strip()

        if user_msg:
            linhas.append(f"Usu√°rio: {user_msg}")
        if ai_msg:
            linhas.append(f"Assistente: {ai_msg}")
            linhas.append("")  # Linha em branco entre turnos

    return "\n".join(linhas) if linhas else "Nenhuma conversa anterior."

def gerar_quick_replies(question, explicacao, history=None, progresso=None):
    opcoes = ["Tenho outra d√∫vida", "Aprofundar este t√≥pico"]
    if isinstance(explicacao, str) and "Quer que eu continue?" in explicacao:
        # Ajuda o usu√°rio a pedir continua√ß√£o explicitamente
        opcoes.insert(0, "Continuar")
    return opcoes

def resposta_link(titulo, url, icone="üìÑ"):
    return f"<br><a class='chip' href='{url}' target='_blank'>{icone} {titulo}</a>"

def resposta_link_externo(titulo, url, icone="üîó"):
    return f"<br><a class='chip' href='{url}' target='_blank'>{icone} {titulo}</a>"

# Detec√ß√£o de cen√°rios simplificada
def detectar_cenario(pergunta: str) -> str:
    pergunta = pergunta.lower()
    
    # Detecta perguntas t√©cnicas sobre sistemas, banco de dados, arquitetura
    termos_tecnicos = [
        "data lake", "crm", "supabase", "postgres", "sql", "rls", "policy", "schema",
        "bronze", "silver", "gold", "lead", "evento", "fun√ß√£o", "trigger", "tabela"
    ]
    
    if any(t in pergunta for t in termos_tecnicos):
        return "duvida_tecnica"
    
    # Detecta perguntas gerais
    if any(p in pergunta for p in [
        "tenho uma d√∫vida", "tenho outra d√∫vida", "minha d√∫vida", "n√£o entendi", "duvida", "d√∫vida", "me explica",
        "poderia explicar", "por que", "como", "o que", "quais", "qual", "explique", "me fale", "exemplo", "caso pr√°tico",
        "me mostre", "me explique", "?"
    ]):
        return "duvida_pontual"
    elif any(p in pergunta for p in [
        "exemplo pr√°tico", "me d√° um exemplo", "passo a passo", "como fazer isso", "como fa√ßo", "me ensina", "ensinar", "me mostre como"
    ]):
        return "exemplo_pratico"
    else:
        return "geral"

def atualizar_progresso(pergunta: str, progresso: dict) -> dict:
    # Sistema simplificado - n√£o usa mais m√≥dulos/aulas
    # Mant√©m estrutura b√°sica para compatibilidade
    if not progresso:
        return {}
    return progresso

# Base de conhecimento - conte√∫do do arquivo transcricoes.txt
# O conte√∫do completo est√° dispon√≠vel via search_engine que indexa o arquivo transcricoes.txt

def generate_answer(question, context="", history=None, tipo_de_prompt=None, is_first_question=True):
    progresso = {}
    
    saudacao = random.choice(GREETINGS) if is_first_question else ""
    fechamento = random.choice(CLOSINGS)
    cenario = detectar_cenario(question)

    mensagem_generica = question.strip().lower()
    saudacoes_vagas = [
        "ol√°", "ola", "oi", "bom dia", "boa tarde", "boa noite", "pode me ajudar?", "oi, tudo bem?",
        "ol√° bom dia", "tudo bem?", "tudo certo?", "como vai?", "voc√™ pode me ajudar?", "me ajuda?", "ol√°, boa noite"
    ]
    apresentacoes_vagas = ["meu nome √©", "sou ", "me apresentando", "me apresento", "me chamo"]

    # Mensagens vagas ("oi", "tudo bem?") devem ir para a LLM
    is_saudacao = (
        mensagem_generica in saudacoes_vagas
        or any(mensagem_generica.startswith(apr) for apr in apresentacoes_vagas)
    )
    if is_saudacao:
        cenario = "saudacao"

    # Construir instruction baseado no cen√°rio
    if cenario == "saudacao":
        instruction = (
            "O usu√°rio enviou uma sauda√ß√£o/mensagem inicial (ex: 'oi', 'tudo bem?'). "
            "Responda de forma acolhedora e objetiva, explique rapidamente como voc√™ pode ajudar com quest√µes sobre "
            "sistemas de CRM, Data Lake, arquitetura de dados, Supabase, PostgreSQL e desenvolvimento de software."
        )
    elif cenario == "duvida_tecnica":
        instruction = (
            "√ìtima pergunta t√©cnica!<br>"
            "Forne√ßa uma explica√ß√£o detalhada e precisa sobre o tema, com exemplos pr√°ticos quando poss√≠vel.<br>"
            "Se quiser aprofundar ou pedir mais exemplos, √© s√≥ pedir!"
        )
    else:
        instruction = (
            "√ìtima pergunta!<br>"
            "Forne√ßa uma explica√ß√£o detalhada sobre o tema, seguida de exemplos pr√°ticos quando poss√≠vel.<br>"
            "Se quiser aprofundar ou pedir mais exemplos, √© s√≥ pedir!"
        )

    prompt = f"""{instruction}

{CONTINUE_GUARDRAILS}

Voc√™ √© um assistente inteligente especializado em ajudar com quest√µes sobre sistemas de CRM, Data Lake, arquitetura de dados e desenvolvimento de software.

Leia atentamente o hist√≥rico da conversa antes de responder, compreendendo o contexto exato da intera√ß√£o atual para garantir precis√£o na sua resposta.

BASE DE CONHECIMENTO DISPON√çVEL:
O sistema possui documenta√ß√£o sobre arquitetura de Data Lake (Bronze ‚Üí Silver ‚Üí Gold), CRM inteligente, RLS Policies para Supabase, fun√ß√µes SQL transacionais, e estruturas de banco de dados para sistemas enterprise.

Hist√≥rico da conversa anterior:
{formatar_historico_para_prompt(history)}

Pergunta atual do usu√°rio:
'{question}'

Utilize o conte√∫do adicional abaixo, se relevante:
{context}
        """
    
    try:
        response = client.messages.create(
            model="MiniMax-M2",
            max_tokens=2048,
            system="Responda SEMPRE em portugu√™s do Brasil.",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        explicacao = response.content[0].text.strip()
        if _looks_truncated(explicacao) or _should_offer_continue(explicacao):
            explicacao = _append_continue_hint(explicacao)
        quick_replies = gerar_quick_replies(question, explicacao, history, progresso)
    except Exception as e:
        print(f"‚ùå Erro ao chamar Minimax API: {e}")
        explicacao = OUT_OF_SCOPE_MSG
        quick_replies = []
        return explicacao, quick_replies, progresso

    if saudacao:
        resposta = f"{saudacao}<br><br>{explicacao}<br><br>{fechamento}"
    else:
        resposta = f"{explicacao}<br><br>{fechamento}"

    return resposta, quick_replies, progresso


# ========== FUN√á√ÉO DE STREAMING PARA WEBSOCKET ==========
async def generate_answer_stream(question, context="", history=None, tipo_de_prompt=None, is_first_question=False):
    """
    Vers√£o streaming da generate_answer para uso com WebSocket.
    Yields dicion√°rios com tipo de conte√∫do e dados.

    Yields:
        dict: {"type": "metadata"|"text"|"complete", "data": {...}}
    """
    progresso = {}
    cenario = detectar_cenario(question)

    # Envia metadados primeiro (progresso)
    yield {
        "type": "metadata",
        "data": {
            "progresso": progresso,
            "cenario": cenario
        }
    }

    # Detecta mensagens vagas
    mensagem_generica = question.strip().lower()
    saudacoes_vagas = [
        "ol√°", "ola", "oi", "bom dia", "boa tarde", "boa noite", "pode me ajudar?", "oi, tudo bem?",
        "ol√° bom dia", "tudo bem?", "tudo certo?", "como vai?", "voc√™ pode me ajudar?", "me ajuda?", "ol√°, boa noite"
    ]
    apresentacoes_vagas = ["meu nome √©", "sou ", "me apresentando", "me apresento", "me chamo"]
    if mensagem_generica in saudacoes_vagas or any(mensagem_generica.startswith(apr) for apr in apresentacoes_vagas):
        cenario = "saudacao"

    # Constr√≥i o prompt baseado no cen√°rio
    if cenario == "saudacao":
        instruction = (
            "O usu√°rio enviou uma sauda√ß√£o/mensagem inicial (ex: 'oi', 'tudo bem?'). "
            "Responda de forma acolhedora e objetiva, explique rapidamente como voc√™ pode ajudar com quest√µes sobre "
            "sistemas de CRM, Data Lake, arquitetura de dados, Supabase, PostgreSQL e desenvolvimento de software."
        )
    elif cenario == "duvida_tecnica":
        instruction = (
            "√ìtima pergunta t√©cnica!<br>"
            "Forne√ßa uma explica√ß√£o detalhada e precisa sobre o tema, com exemplos pr√°ticos quando poss√≠vel.<br>"
        )
    elif cenario in ["duvida_pontual", "exemplo_pratico"]:
        instruction = (
            "√ìtima pergunta!<br>"
            "Forne√ßa uma explica√ß√£o detalhada sobre o tema, seguida de exemplos pr√°ticos quando poss√≠vel.<br>"
        )
    else:
        instruction = ""

    prompt = f"""{instruction}

{CONTINUE_GUARDRAILS}

Voc√™ √© um assistente inteligente especializado em ajudar com quest√µes sobre sistemas de CRM, Data Lake, arquitetura de dados e desenvolvimento de software.

Leia atentamente o hist√≥rico da conversa antes de responder, compreendendo o contexto exato da intera√ß√£o atual para garantir precis√£o na sua resposta.

BASE DE CONHECIMENTO DISPON√çVEL:
O sistema possui documenta√ß√£o sobre arquitetura de Data Lake (Bronze ‚Üí Silver ‚Üí Gold), CRM inteligente, RLS Policies para Supabase, fun√ß√µes SQL transacionais, e estruturas de banco de dados para sistemas enterprise.

Hist√≥rico da conversa anterior:
{formatar_historico_para_prompt(history)}

Pergunta atual do usu√°rio:
'{question}'

Utilize o conte√∫do adicional abaixo, se relevante:
{context}
    """

    try:
        # Chama Minimax com streaming habilitado (via API Anthropic)
        with client.messages.stream(
            model="MiniMax-M2",
            max_tokens=2048,
            system="Responda SEMPRE em portugu√™s do Brasil.",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        ) as stream:
            # Acumula resposta completa
            full_response = ""

            # Itera pelos chunks da resposta
            for text in stream.text_stream:
                full_response += text
                yield {"type": "text", "data": text}

        if _looks_truncated(full_response) or _should_offer_continue(full_response):
            full_response = _append_continue_hint(full_response)

        # Gera quick_replies baseado na resposta
        quick_replies = gerar_quick_replies(question, full_response, history, progresso)

        # Envia dados de conclus√£o
        yield {
            "type": "complete",
            "data": {
                "quick_replies": quick_replies,
                "progresso": progresso,
                "full_response": full_response
            }
        }

    except Exception as e:
        print(f"‚ùå Erro ao fazer streaming da Minimax API: {e}")
        yield {"type": "text", "data": OUT_OF_SCOPE_MSG}
        yield {
            "type": "complete",
            "data": {
                "quick_replies": [],
                "progresso": progresso,
                "error": str(e)
            }
        }

def generate_conversation_summary(messages: list, max_length: int = 500) -> str:
    """
    Gera resumo de uma conversa usando LLM.

    Args:
        messages: Lista de mensagens no formato [{'role': 'user'|'assistant', 'content': '...'}]
        max_length: Comprimento m√°ximo do resumo (padr√£o: 500 caracteres)

    Returns:
        Resumo formatado da conversa
    """
    if not messages:
        return "Conversa vazia."

    # Extrair texto das mensagens
    conversation_text = ""
    for msg in messages:
        role = msg.get('role', '').lower()
        content = msg.get('content', '').strip()
        if content:
            prefix = "Usu√°rio" if role == 'user' else "Assistente"
            conversation_text += f"{prefix}: {content}\n\n"

    if not conversation_text.strip():
        return "Conversa sem conte√∫do textual."

    # Truncar se muito longo (limitar a ~3000 caracteres para o prompt)
    if len(conversation_text) > 3000:
        conversation_text = conversation_text[:3000] + "\n\n[... conversa√ß√£o truncada ...]"

    prompt = f"""
Por favor, crie um resumo conciso desta conversa em portugu√™s brasileiro.

DIRETRIZES:
- M√°ximo de {max_length} caracteres
- 2-3 frases apenas
- Destaque os t√≥picos principais discutidos
- Mencione conclus√µes ou decis√µes importantes
- Se houver pr√≥ximos passos mencionados, inclua-os
- Use linguagem clara e objetiva
- N√£o use markdown ou formata√ß√£o especial

CONVERSA:
{conversation_text}

RESUMO:
"""

    try:
        response = client.messages.create(
            model="MiniMax-M2",
            max_tokens=300,
            system="Voc√™ √© um assistente especializado em criar resumos concisos e √∫teis de conversas. Responda SEMPRE em portugu√™s do Brasil.",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )

        summary = response.content[0].text.strip()

        # Garantir que n√£o excede o limite
        if len(summary) > max_length:
            summary = summary[:max_length-3] + "..."

        return summary

    except Exception as e:
        print(f"‚ùå Erro ao gerar resumo: {e}")
        return f"Erro ao gerar resumo: {str(e)}"

async def generate_conversation_summary_stream(messages: list, max_length: int = 500):
    """
    Gera resumo de uma conversa usando LLM com streaming.

    Args:
        messages: Lista de mensagens no formato [{'role': 'user'|'assistant', 'content': '...'}]
        max_length: Comprimento m√°ximo do resumo (padr√£o: 500 caracteres)

    Yields:
        Chunks de texto do resumo conforme gerado
    """
    if not messages:
        yield "Conversa vazia."
        return

    # Extrair texto das mensagens
    conversation_text = ""
    for msg in messages:
        role = msg.get('role', '').lower()
        content = msg.get('content', '').strip()
        if content:
            prefix = "Usu√°rio" if role == 'user' else "Assistente"
            conversation_text += f"{prefix}: {content}\n\n"

    if not conversation_text.strip():
        yield "Conversa sem conte√∫do textual."
        return

    # Truncar se muito longo (limitar a ~3000 caracteres para o prompt)
    if len(conversation_text) > 3000:
        conversation_text = conversation_text[:3000] + "\n\n[... conversa√ß√£o truncada ...]"

    prompt = f"""
Por favor, crie um resumo conciso desta conversa em portugu√™s brasileiro.

DIRETRIZES:
- M√°ximo de {max_length} caracteres
- 2-3 frases apenas
- Destaque os t√≥picos principais discutidos
- Mencione conclus√µes ou decis√µes importantes
- Se houver pr√≥ximos passos mencionados, inclua-os
- Use linguagem clara e objetiva
- N√£o use markdown ou formata√ß√£o especial

CONVERSA:
{conversation_text}

RESUMO:
"""

    try:
        # Usar streaming similar ao generate_answer_stream
        with client.messages.stream(
            model="MiniMax-M2",
            max_tokens=300,
            system="Voc√™ √© um assistente especializado em criar resumos concisos e √∫teis de conversas. Responda SEMPRE em portugu√™s do Brasil.",
            messages=[
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        ) as stream:
            full_text = ""
            # Usar text_stream para evitar ThinkingBlock e outros tipos de chunk
            for text in stream.text_stream:
                full_text += text
                yield text

        # Garantir que n√£o excede o limite
        if len(full_text) > max_length:
            yield "... (resumo truncado)"

    except Exception as e:
        print(f"‚ùå Erro ao gerar resumo stream: {e}")
        yield f"Erro ao gerar resumo: {str(e)}"
